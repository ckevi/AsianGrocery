{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to Setup Shop in The Twin Cities\n",
    "### By Kevin Chou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to help or give potential investors or business owners initial guidance on where to open a new Asian Grocery Market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import neccessary libraries\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.')\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the zipatlas.com website to get the asian population data in MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.1.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.2.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.3.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.4.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.5.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.6.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.7.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.8.htm',\n",
    "        'http://zipatlas.com/us/mn/zip-code-comparison/percentage-asian-population.9.htm']\n",
    "\n",
    "#loading empty array for board members\n",
    "table = []\n",
    "asians = []\n",
    "#Loop through our URLs we loaded above\n",
    "for b in urls:\n",
    "    html = requests.get(b).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "#identify table we want to scrape\n",
    "    table = soup.find('table', {'rules' : \"all\"})\n",
    "#try clause to skip any companies with missing/empty board member tables\n",
    "    try:\n",
    "#loop through table, grab each of the 4 columns shown (try one of the links yourself to see the layout)\n",
    "        for row in table.find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) == 7:\n",
    "                 asians.append((cols[0].text.strip(), cols[1].text.strip(), cols[2].text.strip(), cols[3].text.strip(),\n",
    "                         cols[4].text.strip(),cols[5].text.strip(),cols[6].text.strip()))\n",
    "    except: pass  \n",
    "COLUMNS = ['Nbr','Zipcodes','Location', 'city', 'Population', '% Asians', 'National Rank']\n",
    "df = pd.DataFrame(asians, columns=COLUMNS).drop(0, axis=0)\n",
    "df = df[df.Nbr != '#']\n",
    "del df['Nbr']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with split value columns \n",
    "new = df[\"Location\"].str.split(\", \", n = 1, expand = True) \n",
    "# making separate first name column from new data frame \n",
    "df[\"Latitude\"]= new[0]  \n",
    "# making separate last name column from new data frame \n",
    "df[\"Longitude\"]= new[1]  \n",
    "# Dropping old Name columns \n",
    "df.drop(columns =[\"Location\"], inplace = True) \n",
    "new = df[\"city\"].str.split(\", \", n = 1, expand = True) \n",
    "df[\"City\"]= new[0]    \n",
    "df[\"State\"]= new[1]    \n",
    "df.drop(columns =[\"city\"], inplace = True) \n",
    "# df display \n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to file for safe keeping\n",
    "df.to_csv('/Users/Mcair/Desktop/projects/DS_Code/MNAsians.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import MNAsians.csv if file already exists and not scraping\n",
    "df = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/MNAsians.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Population'] = df['Population'].astype('str')\n",
    "df['Population'] = df['Population'].str.replace(',', '')\n",
    "df['Population'] = df['Population'].astype('float64')\n",
    "df['Zipcodes'] = df['Zipcodes'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Twin Cities zip codes to use for filtering\n",
    "tczip = pd.read_csv('TCZipCodes.csv')\n",
    "tczip['TCZipCodes'] = tczip['TCZipCodes'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join previous dataset with TCZipCodes to filter out zipcode not belonging to the Twin Cities\n",
    "dfnew = pd.merge(tczip, df, how='inner', left_on = 'TCZipCodes', right_on = 'Zipcodes')\n",
    "dfnew.drop(columns =[\"TCZipCodes\"], inplace = True) \n",
    "dfnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew['Latitude'] = dfnew['Latitude'].astype('float64')\n",
    "dfnew['Longitude'] = dfnew['Longitude'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the density of the Asian population in the Twin Cities zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download mn file\n",
    "#!wget --quiet https://raw.githubusercontent.com/OpenDataDE/State-zip-code-GeoJSON/master/mn_minnesota_zip_codes_geo.min.json\n",
    "#Manually renamed to mn_geo.json    \n",
    "with open('mn_geo.json') as json_data:\n",
    "    mn_geo = json.load(json_data)\n",
    "\n",
    "print('GeoJSON file loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter zip codes where population is > 10,000 Asians.  \n",
    "df = dfnew[dfnew['Population'] > 10000]\n",
    "df.to_csv('/Users/Mcair/Desktop/projects/DS_Code/ZipCodeAsian10000.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cleaned file if needed.\n",
    "df = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/ZipCodeAsian10000.csv')\n",
    "df['Population'].astype(str)\n",
    "df['Population'] = df['Population'].astype('float64')\n",
    "df['Zipcodes'] = df['Zipcodes'].astype('str')\n",
    "df['Latitude'] = df['Latitude'].astype('float64')\n",
    "df['Longitude'] = df['Longitude'].astype('float64')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the map\n",
    "tc = folium.Map(location=[44.9778, -93.2650], zoom_start=10)#,tiles='Mapbox Bright')\n",
    " \n",
    "    # create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\n",
    "threshold_scale = np.linspace(df['Population'].min(),\n",
    "                              df['Population'].max(),\n",
    "                              6, dtype=int)\n",
    "\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n",
    "\n",
    "# Add the color for the chloropleth:\n",
    "tc.choropleth(\n",
    " geo_data=mn_geo,\n",
    " data=df,\n",
    " columns=['Zipcodes', 'Population'],\n",
    " key_on='feature.properties.ZCTA5CE10',\n",
    " threshold_scale=threshold_scale,\n",
    " fill_color='BuPu',\n",
    " fill_opacity=0.8,\n",
    " line_opacity=0.1,\n",
    " legend_name='Population'\n",
    ")\n",
    "tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Yelp to search for Asian Grocery stores in the Twin Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Yelp API variables\n",
    "import requests\n",
    "\n",
    "ClientID='OdcNPOaZPK-1Rw94P1wEMg'\n",
    "\n",
    "api_key='8yezNH02SBHwHUnl4_GnP1KOlIjRy8pwV07jxGZxBf9dfL63nd0pe5jR_E8PmJ0UPQEkn0lYAUtvnH0cry-QfleuPP6bxqozijepm4EiIca_otHzhOxykFR4nR_4XHYx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_LIMIT = 50 # limit of number of venues returned by Yelp API\n",
    "#OFFSET=50\n",
    "def getBusinesses(Zcodes, latudes, lotudes):\n",
    "    \n",
    "    biz_list = []\n",
    "    #rad=500\n",
    "    term = 'Asian Grocery'\n",
    "    #term = ''\n",
    "    \n",
    "    for code, lat, lng in zip(Zcodes, latudes, lotudes):\n",
    "    # create the API request URL      \n",
    "        url = 'https://api.yelp.com/v3/businesses/search'     \n",
    "        \n",
    "        headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "        }\n",
    "        \n",
    "        url_params = {\n",
    "                'term': term.replace(' ', '+'),\n",
    "                'latitude': lat,\n",
    "                'longitude': lng,\n",
    "                'radius': 10000,\n",
    "                'limit': SEARCH_LIMIT\n",
    "        }\n",
    "        # make the GET request\n",
    "        results = requests.get(url, headers=headers, params=url_params).json()['businesses']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        biz_list.append([(\n",
    "            code, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['name'],\n",
    "            v['location']['city'],\n",
    "            v['location']['zip_code'],\n",
    "            v['coordinates']['latitude'], \n",
    "            v['coordinates']['longitude']) for v in results])\n",
    "    \n",
    "   # nearby_biz = pd.DataFrame(pd.np.empty((0, 6)))    \n",
    "    nearby_biz = pd.DataFrame([item for biz_list in biz_list for item in biz_list])\n",
    "    nearby_biz.columns = ['Zipcodes', \n",
    "                  'Zipcode Latitude', \n",
    "                  'Zipcode Longitude', \n",
    "                  'Business',\n",
    "                  'City', \n",
    "                  'Biz Zipcode',\n",
    "                  'Biz Latitude', \n",
    "                  'Biz Longitude']\n",
    "    \n",
    "    return(nearby_biz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = getBusinesses(Zcodes=df['Zipcodes'],\n",
    "                               latudes=df['Latitude'],\n",
    "                               lotudes=df['Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output to csv for further review and cleanup\n",
    "businesses.to_csv('/Users/Mcair/Desktop/projects/DS_Code/tcbusinesses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the cleaned Twin Cities Asian Store file\n",
    "Astores = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/tcbusiness1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores['City'] = Astores['City'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AStores = Astores.filter(['Business', 'Zipcodes.1','City','Biz Latitude', 'Biz Longitude'], axis=1)\n",
    "AStores.rename({'Zipcodes.1': 'Zipcodes'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AStores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AStores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate reocords\n",
    "AStores = AStores.drop_duplicates(subset=['Business', 'City','Biz Latitude', 'Biz Longitude'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AStores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AStores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AStores.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to file for manual upload if needed\n",
    "AStores.to_csv('/Users/Mcair/Desktop/projects/DS_Code/AStores.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following map illustrates the locations of established Asian markets and the density of Asians in the areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lat, lng, name,cty, zipc in zip(AStores['Biz Latitude'], AStores['Biz Longitude'], AStores['Business'],AStores['City'],AStores['Zipcodes']):\n",
    "    label = '{},{},{}'.format(name,cty,zipc)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "\n",
    "    #folium.Marker([lat, lng], popup=label, icon=folium.Icon(color='red')).add_to(tc)\n",
    "\n",
    "    folium.CircleMarker([lat, lng],radius=5,popup=label,color='yellow',fill=True,fill_color='red',\n",
    "                        fill_opacity=0.7,\n",
    "                        parse_html=False).add_to(tc)  \n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we use the cordinates of the Asian markets to search for nearby venues using data from Foursquare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup Foursquare credentials for API calls\n",
    "CLIENT_ID = 'CNH2BLPEMVP2JSUSH0RNO3TXIHTKVWSEFXIJ2VS4OM5CTMMO' # your Foursquare ID\n",
    "CLIENT_SECRET = 'PHHVYR1MUJC2UKCRJXSSUFNFJMFIXHF45YDHLYK422QQE55R' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LIMIT = 100 # limit of number of venues returned by Foursquare API\n",
    "\n",
    "def getNearbyVenues(Zcodes, latudes, lotudes, radius=800):\n",
    "    venues_list=[]\n",
    "    for code, lat, lng in zip(Zcodes, latudes, lotudes):\n",
    "    #print(name)     \n",
    "    # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            code, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Zipcodes', \n",
    "                  'Zipcode Latitude', \n",
    "                  'Zipcode Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcvenues = getNearbyVenues(Zcodes=AStores['Zipcodes'],\n",
    "                           latudes=AStores['Biz Latitude'],\n",
    "                           lotudes=AStores['Biz Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcvenues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tcvenues.shape)\n",
    "tcvenues.to_csv('/Users/Mcair/Desktop/projects/DS_Code/tcvenues.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcvenues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcvenues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the tcvenues file if we do not need to requery Foursquare\n",
    "tcvenues = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/tcvenues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcvenues.groupby('Zipcodes').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get venue counts for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venuecount = tcvenues.groupby(['Zipcodes']).size().reset_index(name='Zipcodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venuecount.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venuecount.plot(kind='bar',x='Zipcodes',y='counts', title='Zip Code Venue Counts',width = .8,figsize=(10,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(tcvenues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "tc_onehot = pd.get_dummies(tcvenues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "tc_onehot['Zipcodes'] = tcvenues['Zipcodes'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [tc_onehot.columns[-1]] + list(tc_onehot.columns[:-1])\n",
    "tc_onehot = tc_onehot[fixed_columns]\n",
    "\n",
    "tc_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_grouped = tc_onehot.groupby('Zipcodes').mean().reset_index()\n",
    "tc_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "\n",
    "for zipc in tc_grouped['Zipcodes']:\n",
    "    print(\"----\"+str(zipc)+\"----\")\n",
    "    temp = tc_grouped[tc_grouped['Zipcodes'] == zipc].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[1:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Get the common venues around each of the established Asian Grocery Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Zipcodes']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Zipcodes'] = tc_grouped['Zipcodes']\n",
    "\n",
    "for ind in np.arange(tc_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(tc_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "tc_grouped_clustering = tc_grouped.drop('Zipcodes', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(tc_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add clustering labels\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "tc_merged = df\n",
    "#df['Zipcodes'] = df['Zipcodes'].astype('int64')\n",
    "tcmerged = pd.merge(tc_merged, neighborhoods_venues_sorted.set_index('Zipcodes'), how='inner', on='Zipcodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged['Zipcodes'] = tcmerged['Zipcodes'].astype('str')\n",
    "tcmerged['Latitude'] = tcmerged['Latitude'].astype('float')\n",
    "tcmerged['Longitude'] = tcmerged['Longitude'].astype('float')\n",
    "\n",
    "tcmerged.drop(['State', 'National Rank'], axis = 1, inplace = True, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged#.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = tcmerged[['Cluster Labels', '1st Most Common Venue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.crosstab(cdf['Cluster Labels'],cdf['1st Most Common Venue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Venues in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "cdf.plot.bar(width = 1,figsize=(15,8),stacked=False)\n",
    "plt.legend(title='Common Venues')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdfc = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/ZipCodeAsian10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdfc['Zipcodes'] = xdfc['Zipcodes'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the map\n",
    "clustermap = folium.Map(location=[44.9778, -93.2650], zoom_start=10)#,tiles='Mapbox Bright')\n",
    " \n",
    "    # create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\n",
    "threshold_scale = np.linspace(xdfc['Population'].min(),\n",
    "                              xdfc['Population'].max(),\n",
    "                              6, dtype=int)\n",
    "\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n",
    "\n",
    "# Add the color for the chloropleth:\n",
    "clustermap.choropleth(\n",
    " geo_data=mn_geo,\n",
    " data=xdfc,\n",
    " columns=['Zipcodes', 'Population'],\n",
    " key_on='feature.properties.ZCTA5CE10',\n",
    " threshold_scale=threshold_scale,\n",
    " fill_color='BuPu',\n",
    " fill_opacity=0.8,\n",
    " line_opacity=0.1,\n",
    " legend_name='Population'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(tcmerged['Latitude'], tcmerged['Longitude'], tcmerged['Zipcodes'], tcmerged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=6,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(clustermap)\n",
    "       \n",
    "clustermap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster 0 - Asian Restuarants and Misc Venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged.loc[tcmerged['Cluster Labels'] == 0, tcmerged.columns[[0] + list(range(5, tcmerged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster 1 - Rich in a variety of Venues: Ethinic Restaurants, Bars, Cafes, Groceries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged.loc[tcmerged['Cluster Labels'] == 1, tcmerged.columns[[0] + list(range(5, tcmerged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster 2 - Entertainment, C-Store, Liquor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged.loc[tcmerged['Cluster Labels'] == 2, tcmerged.columns[[0] + list(range(5, tcmerged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster 3 - Park, Asian Restaurant, Liquor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmerged.loc[tcmerged['Cluster Labels'] == 3, tcmerged.columns[[0] + list(range(5, tcmerged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Map to illustrate where opportunities exists in relation to the current establishments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cleaned file to and get high density zipcode that don't have Asian Grocery Stores.\n",
    "xdf = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/ZipCodeAsian10000.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add attribute to distinguish record dataset\n",
    "xdf['in_xdf']='yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.drop(columns =['National Rank','State'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the current Asian stores\n",
    "Astores = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/AStores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add attribute to distinguish record dataset\n",
    "Astores['inAstores']='yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores.drop(columns =['Business','Biz Latitude', 'Biz Longitude', 'City'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores = Astores.drop_duplicates()\n",
    "#Astores = Astores.drop_duplicates(subset=['Business', 'City','Biz Latitude', 'Biz Longitude'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Astores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join datasets\n",
    "xdfx = pd.merge(Astores, xdf, how='right', on = 'Zipcodes')\n",
    "#dfnew.drop(columns =[\"TCZipCodes\"], inplace = True) \n",
    "xdfx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdfx.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdfx['Zipcodes'] = xdfx['Zipcodes'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdfx.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out zipcodes that already have Asian Stores\n",
    "xdfx = xdfx.loc[(xdfx['inAstores'] != 'yes') & (xdfx['in_xdf'] == 'yes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There exist 71 Zip codes that have Asian Population of over 10K that do not have Asian Grocery Stores\n",
    "xdfx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the map\n",
    "newst = folium.Map(location=[44.9778, -93.2650], zoom_start=10)#,tiles='Mapbox Bright')\n",
    " \n",
    "    # create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\n",
    "threshold_scale = np.linspace(xdfx['Population'].min(),\n",
    "                              xdfx['Population'].max(),\n",
    "                              6, dtype=int)\n",
    "\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n",
    "\n",
    "# Add the color for the chloropleth:\n",
    "newst.choropleth(\n",
    " geo_data=mn_geo,\n",
    " data=xdfx,\n",
    " columns=['Zipcodes', 'Population'],\n",
    " key_on='feature.properties.ZCTA5CE10',\n",
    " threshold_scale=threshold_scale,\n",
    " fill_color='BuPu',\n",
    " fill_opacity=0.8,\n",
    " line_opacity=0.1,\n",
    " legend_name='Population'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lat, lng,cty, zipc in zip(xdfx['Latitude'], xdfx['Longitude'],xdfx['City'],xdfx['Zipcodes']):\n",
    "    label = '{},{}'.format(cty,zipc)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "\n",
    "    #folium.Marker([lat, lng], popup=label, icon=folium.Icon(color='red')).add_to(tc)\n",
    "\n",
    "    folium.CircleMarker([lat, lng],radius=8,popup=label,color='red',fill=False,\n",
    "                        fill_opacity=0.7,parse_html=False).add_to(newst)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Astores file to map the current establishments.\n",
    "AStores = pd.read_csv('/Users/Mcair/Desktop/projects/DS_Code/AStores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map of Zip Code Opportunities that can support new Asian Grocery Stores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lat, lng, name,cty, zipc in zip(AStores['Biz Latitude'], AStores['Biz Longitude'], AStores['Business'],AStores['City'],AStores['Zipcodes']):\n",
    "    label = '{},{},{}'.format(name,cty,zipc)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "\n",
    "    #folium.Marker([lat, lng], popup=label, icon=folium.Icon(color='red')).add_to(tc)\n",
    "\n",
    "    folium.CircleMarker([lat, lng],radius=5,popup=label,color='yellow',fill=True,fill_color='red',\n",
    "                        fill_opacity=0.7,\n",
    "                        parse_html=False).add_to(newst)  \n",
    "newst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The red circles and popup represent the City and Zip Code where there is opportunity to setup shop as it visualizes the population desnsity of the zip code. The yellow/red circles are the current Asian grocery establishments. As you can see there are many zip codes that we can futher investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For example, the St. Paul zip code 55110 is has a high density of Asians but there are not Asian Grocery establishments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
